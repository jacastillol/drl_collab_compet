{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT: DRL for Collaborative and Competitive Agents\n",
    "\n",
    "## Description and Implementation\n",
    "\n",
    "The code is written in four files:\n",
    "\n",
    "1. `model.py` has to inherited classes from `nn.Module` from torch. One called `Actor` and the other called `Critic`. Both are multilayer and is posible to change the number of nodes in each layer.\n",
    "1. `ddpg_agent.py` has an `Agent` class which handles the learning mechanism over a structure Actor-Critic. Furthermore this file contains two more classes, a class `ReplayBuffer` and a class `OUNoise`, to handle experience replay and exploration-exploitation dilemma respectively.\n",
    "1. `ddpg_interact.py` has a general function `maddpg()` that handles the interation between the `Agent` and the `UnityEnvironment`.\n",
    "1. `learn_and_prove.py` has the main program and handles all the parameters present in the `config.ini` file needed to run all the classes, also all the input option from the command line.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The __DDPG__ algorithm is adapted to multi-agent case __MDDPG__. Each agent has its own experience.\n",
    "\n",
    "### About Hyperparameters\n",
    "```python\n",
    "config = {\n",
    "    n_episodes:      2000\n",
    "    max_t:           1000\n",
    "    print_every:       50\n",
    "    SEED:               0\n",
    "    BUFFER_SIZE:      1e5\n",
    "    BATCH_SIZE:       128\n",
    "    UPDATE_EVERY:       1\n",
    "    GAMMA:           0.99\n",
    "    SIGMA:           0.20\n",
    "    TAU:             6e-2\n",
    "    LR_ACTOR:        1e-4\n",
    "    LR_CRITIC:       1e-3\n",
    "    WEIGHT_DECAY:       0\n",
    "    FC1_ACTOR:        400\n",
    "    FC2_ACTOR:        300\n",
    "    FC1_CRITIC:       400\n",
    "    FC2_CRITIC:       300\n",
    "    }\n",
    "```\n",
    "\n",
    "### About Model Architectures\n",
    "```\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=48, out_features=400, bias=True)\n",
    "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
    "  (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
    ")\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Linear-1                  [-1, 400]          19,600\n",
    "            Linear-2                  [-1, 300]         120,300\n",
    "            Linear-3                    [-1, 2]             602\n",
    "================================================================\n",
    "Total params: 140,502\n",
    "Trainable params: 140,502\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.01\n",
    "Params size (MB): 0.54\n",
    "Estimated Total Size (MB): 0.54\n",
    "----------------------------------------------------------------\n",
    "Critic(\n",
    "  (fcs1): Linear(in_features=48, out_features=400, bias=True)\n",
    "  (fc2): Linear(in_features=404, out_features=300, bias=True)\n",
    "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
    ")\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Linear-1                  [-1, 400]          19,600\n",
    "            Linear-2                  [-1, 300]         121,500\n",
    "            Linear-3                    [-1, 1]             301\n",
    "================================================================\n",
    "Total params: 141,401\n",
    "Trainable params: 141,401\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 0.01\n",
    "Params size (MB): 0.54\n",
    "Estimated Total Size (MB): 0.55\n",
    "\n",
    "```\n",
    "\n",
    "## Plot of Rewards\n",
    "![learning curve](images/drl001_learning_curve.png)\n",
    "\n",
    "Output console:\n",
    "```\n",
    "Episode   50/2000 || Score 0.10000 || Last avg. scores 0.00580 || Best avg. score 0.00580 \n",
    "Episode  100/2000 || Score 0.00000 || Last avg. scores 0.01130 || Best avg. score 0.01284 \n",
    "Episode  150/2000 || Score 0.00000 || Last avg. scores 0.02390 || Best avg. score 0.02490 \n",
    "Episode  200/2000 || Score 0.10000 || Last avg. scores 0.03390 || Best avg. score 0.03390 \n",
    "Episode  250/2000 || Score 0.09000 || Last avg. scores 0.06670 || Best avg. score 0.06670 \n",
    "Episode  300/2000 || Score 0.19000 || Last avg. scores 0.08430 || Best avg. score 0.08740 \n",
    "Episode  350/2000 || Score 0.10000 || Last avg. scores 0.08140 || Best avg. score 0.08810 \n",
    "Episode  400/2000 || Score 0.20000 || Last avg. scores 0.09620 || Best avg. score 0.09620 \n",
    "Episode  450/2000 || Score 0.00000 || Last avg. scores 0.11530 || Best avg. score 0.11640 \n",
    "Episode  500/2000 || Score 0.10000 || Last avg. scores 0.12570 || Best avg. score 0.12770 \n",
    "Episode  550/2000 || Score 0.10000 || Last avg. scores 0.14890 || Best avg. score 0.14890 \n",
    "Episode  600/2000 || Score 0.10000 || Last avg. scores 0.14450 || Best avg. score 0.15400 \n",
    "Episode  650/2000 || Score 0.10000 || Last avg. scores 0.15160 || Best avg. score 0.16440 \n",
    "Episode  700/2000 || Score 0.50000 || Last avg. scores 0.23530 || Best avg. score 0.23530 \n",
    "Episode  750/2000 || Score 0.40000 || Last avg. scores 0.32340 || Best avg. score 0.32340 \n",
    "Episode  800/2000 || Score 0.60000 || Last avg. scores 0.36760 || Best avg. score 0.36760 \n",
    "Episode  850/2000 || Score 0.10000 || Last avg. scores 0.45130 || Best avg. score 0.45630 \n",
    "Episode  888/2000 || Score 2.10000 || Last avg. scores 0.50820 || Best avg. score 0.50820 \n",
    "Environment solved in 888 episodes!\tAverage Score: 0.51\tin 1975.80 secs\n",
    "```\n",
    "\n",
    "## Ideas of Future Works\n",
    "\n",
    "* It would be interesting to apply this MADDPG to a more complex environment where several agents can be analyzed.\n",
    "* To explore competition and competitive experices to improve the algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
